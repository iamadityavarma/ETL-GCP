# GCP ETL Pipeline - Environment Variables Example
# Copy this file to .env and fill in your actual values

# ============================================================================
# Database Configuration (PostgreSQL)
# ============================================================================
DB_HOST=your-db-host.example.com
DB_HOST_PUBLIC=34.123.45.67  # Optional: Public IP fallback
DB_PORT=5432
DB_NAME=your_database_name
DB_USER=your_db_user
DB_PASSWORD=your_db_password

# ============================================================================
# BigQuery Configuration
# ============================================================================
BQ_PROJECT=your-gcp-project-id
BQ_DATASET=your_dataset_name
BQ_TABLE=cleaned_cdc_chronic_disease

# ============================================================================
# API Configuration
# ============================================================================
# CDC Chronic Disease Indicators API
CDC_API_URL=https://data.cdc.gov/resource/g4ie-h725.csv?$limit=1000000

# ============================================================================
# Table Names
# ============================================================================
STAGING_CDC_TABLE=staging.staging_cdc_chronic_disease

# ============================================================================
# GCS Transient Storage (REQUIRED for GCP)
# ============================================================================
# GCS bucket for temporary chunk storage
GCS_BUCKET=your-etl-transient-bucket

# Prefix/folder in bucket (optional)
GCS_PREFIX=transient/

# Number of rows per chunk
API_CHUNK_SIZE=50000

# Keep files after success (true for debugging, false for production)
KEEP_TRANSIENT_FILES=false

# ============================================================================
# Data Validation Settings
# ============================================================================
# Minimum expected rows in BigQuery table
MIN_EXPECTED_ROWS=100000

# Maximum age of data in hours (for freshness check)
MAX_DATA_AGE_HOURS=168  # 7 days
